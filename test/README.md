To run the experiments, follow the steps below. You can also find the visualization in experiment_flow pdf file. 

1. Run random_cmd_gen.py to create a random ordering of running create view commands for 5 sets into evaluation_experiment.txt.

2. Run java -cp "lib2/*:src/" main.Main evaluation_experiment.txt to measure the average creation time for view declarations along with the size of the views. The logs will be written into experiment_output.txt.

3. Run create_result_ret.py to go over experiment_output.txt and create performance plots that will be saved under the evaluation_plots directory.

4. Run convert_create_to_baseline.py to write the baseline equivalents of queries to baseline_queries.txt.

5. Run java -cp "lib2/*:src/" main.Main baseline_queries.txt to execute the baseline queries in the middleware. The logs will be written into baseline_output.txt.

6. Run baseline_result_ret.py to go over baseline_output.txt and create performance plots that will be saved under the evaluation_plots directory.

7. Run random_use_query_gen.py to create basic MATCH(n) LOCAL use queries with the same random order first generated by random_cmd_gen.py. The final commands (containing first creating all the views in the system and then running the use queries in a random order and for 5 iterations) will be written into use_experiment.txt.

8. Run java -cp "lib2/*:src/" main.Main use_experiment.txt to execute the use queries in the middleware. The logs will be written into use_output.txt.

9. Finally, run use_result_ret.py to go over use_output.txt and create performance plots for USE queries that will be saved under the evaluation_plots directory.